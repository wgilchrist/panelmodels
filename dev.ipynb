{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numba as nb\n",
    "from numba import njit, prange\n",
    "from concurrent.futures import ThreadPoolExecutor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_synthetic_data(n=6500000, n_groups=10000, n_covariates=10, n_equations=10, rng_seed=42):\n",
    "    rng = np.random.default_rng(rng_seed)\n",
    "    group_ids = rng.integers(0, n_groups, size=n)\n",
    "\n",
    "    X = rng.uniform(-3, 3, size=(n, n_covariates))\n",
    "    alpha_true = rng.normal(0, 1, size=(n_equations, n_groups))\n",
    "    beta_true = rng.normal(0, 1, size=(n_equations, n_covariates))\n",
    "    \n",
    "    eta = np.zeros((n, n_equations))\n",
    "    for eqn_id in range(n_equations):\n",
    "        for i in range(n):\n",
    "            group_idx = group_ids[i]\n",
    "            eta[i, eqn_id] = alpha_true[eqn_id, group_idx] + X[i] @ beta_true[eqn_id].T\n",
    "\n",
    "    mu = np.exp(eta)\n",
    "    Y = rng.poisson(mu)\n",
    "    \n",
    "    return Y, X, group_ids, alpha_true, beta_true\n",
    "\n",
    "n = 6_500_000\n",
    "n_groups = 10000\n",
    "n_covariates = 10\n",
    "n_equations = 1\n",
    "Y, X, group_ids, alpha_true, beta_true = generate_synthetic_data(n, n_groups, n_covariates, n_equations, rng_seed=np.random.randint(10_000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def fit_poisson_fe_exact(X, y, group_ids, beta0=None, l2=0.0, \n",
    "                         max_iter=50, tol=1e-8, eps=1e-12, verbose=False):\n",
    "    \"\"\"\n",
    "    Poisson regression with group fixed effects, using IRLS with exact FE absorption.\n",
    "    \"\"\"\n",
    "    X = np.asarray(X, dtype=np.float64)\n",
    "    y = np.asarray(y, dtype=np.float64)\n",
    "    g = np.asarray(group_ids, dtype=np.int64)\n",
    "    n, p = X.shape\n",
    "    m = int(g.max()) + 1\n",
    "    \n",
    "    # Precompute group sums of y\n",
    "    T = np.bincount(g, weights=y, minlength=m)\n",
    "\n",
    "    beta = np.zeros(p) if beta0 is None else beta0.copy()\n",
    "\n",
    "    for it in range(max_iter):\n",
    "        # Step 1: Given beta, get alpha by profiling out\n",
    "        eta0 = X @ beta\n",
    "        S = np.bincount(g, weights=np.exp(eta0), minlength=m)\n",
    "        alpha = np.log(np.maximum(T, eps)) - np.log(np.maximum(S, eps))\n",
    "        \n",
    "        eta = eta0 + alpha[g]\n",
    "        mu = np.exp(eta)\n",
    "        \n",
    "        # Step 2: weights & pseudo-response for IRLS\n",
    "        W = mu\n",
    "        z = eta + (y - mu) / np.maximum(mu, eps)\n",
    "\n",
    "        # Step 3: weighted demeaning within groups\n",
    "        # Weighted group means\n",
    "        Wg = np.bincount(g, weights=W, minlength=m)\n",
    "        Xw = X * W[:, None]\n",
    "        Xgw = np.vstack([np.bincount(g, weights=Xw[:, j], minlength=m) for j in range(p)]).T\n",
    "        zg = np.bincount(g, weights=W * z, minlength=m)\n",
    "\n",
    "        # demeaned variables\n",
    "        X_tilde = X - Xgw[g] / Wg[g, None]\n",
    "        z_tilde = z - zg[g] / Wg[g]\n",
    "\n",
    "        # Step 4: weighted least squares on demeaned vars\n",
    "        WX_tilde = X_tilde * np.sqrt(W)[:, None]\n",
    "        wz_tilde = z_tilde * np.sqrt(W)\n",
    "        XtWX = WX_tilde.T @ WX_tilde\n",
    "        XtWz = WX_tilde.T @ wz_tilde\n",
    "\n",
    "        if l2 > 0:\n",
    "            XtWX.flat[::p+1] += l2\n",
    "            XtWz += -l2 * beta  # ridge penalty gradient\n",
    "\n",
    "        step = np.linalg.solve(XtWX, XtWz)\n",
    "        \n",
    "        # Check convergence\n",
    "        if np.linalg.norm(step - beta) < tol * (1 + np.linalg.norm(beta)):\n",
    "            beta = step\n",
    "            break\n",
    "\n",
    "        beta = step\n",
    "        \n",
    "        if verbose:\n",
    "            nll = -(y @ eta - mu.sum())\n",
    "            print(f\"iter {it:02d}  nll={nll:.6f}  |Δβ|={np.linalg.norm(step-beta):.2e}\")\n",
    "\n",
    "    # Final alpha\n",
    "    eta0 = X @ beta\n",
    "    S = np.bincount(g, weights=np.exp(eta0), minlength=m)\n",
    "    alpha = np.log(np.maximum(T, eps)) - np.log(np.maximum(S, eps))\n",
    "    \n",
    "    return beta, alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def _soft_threshold(x, lam):\n",
    "    if x > lam: return x - lam\n",
    "    if x < -lam: return x + lam\n",
    "    return 0.0\n",
    "\n",
    "def fit_poisson_fe_lasso_normalized(\n",
    "    X, y, group_ids,\n",
    "    lambda_norm=1.0,        # user-scale penalty; ~1 starts zeroing coefs\n",
    "    l2=0.0,                 # optional ridge on beta\n",
    "    beta0=None,\n",
    "    max_outer=50,\n",
    "    max_inner=100,\n",
    "    tol=1e-6,\n",
    "    inner_tol=1e-8,\n",
    "    eps=1e-12,\n",
    "    verbose=False\n",
    "):\n",
    "    \"\"\"\n",
    "    Poisson FE (alphas absorbed exactly) + L1 on beta with a normalized lambda.\n",
    "    We calibrate lambda so that lambda_norm=1 corresponds to lambda_max at beta=0\n",
    "    in the first IRLS subproblem using weighted, FE-demeaned features.\n",
    "\n",
    "    Returns: beta (p,), alpha (m,), info dict\n",
    "    \"\"\"\n",
    "    X = np.asarray(X, dtype=np.float64)\n",
    "    y = np.asarray(y, dtype=np.float64)\n",
    "    g = np.asarray(group_ids, dtype=np.int64)\n",
    "    n, p = X.shape\n",
    "    m = int(g.max()) + 1\n",
    "\n",
    "    # Sufficient stats for alpha profile\n",
    "    T = np.bincount(g, weights=y, minlength=m)\n",
    "\n",
    "    # Start at beta=0 unless provided\n",
    "    beta_std = np.zeros(p) if beta0 is None else beta0.astype(np.float64).copy()\n",
    "    s = np.ones(p, dtype=np.float64)  # column scales for standardization (filled after calibration)\n",
    "    lambda_calibrated = None          # actual lambda used in CD after calibration\n",
    "\n",
    "    def profile_alpha(beta):\n",
    "        eta0 = X @ (beta / s)  # because beta stored in \"std\" space\n",
    "        S = np.bincount(g, weights=np.exp(eta0), minlength=m)\n",
    "        alpha = np.log(np.maximum(T, eps)) - np.log(np.maximum(S, eps))\n",
    "        return alpha, eta0\n",
    "\n",
    "    # --- One-time calibration at beta=0 to define s and lambda_max ---\n",
    "    beta_std[:] = 0.0\n",
    "    alpha, eta0 = profile_alpha(beta_std)\n",
    "    eta = eta0 + alpha[g]\n",
    "    mu = np.exp(eta)\n",
    "    W = mu\n",
    "    z = eta + (y - mu) / np.maximum(mu, eps)\n",
    "\n",
    "    # weighted FE demeaning\n",
    "    Wg = np.maximum(np.bincount(g, weights=W, minlength=m), eps)\n",
    "    Xw = X * W[:, None]\n",
    "    Xgw = np.vstack([np.bincount(g, weights=Xw[:, j], minlength=m) for j in range(p)]).T\n",
    "    zg  = np.bincount(g, weights=W * z, minlength=m)\n",
    "    X_tilde = X - Xgw[g] / Wg[g, None]\n",
    "    z_tilde = z - zg[g] / Wg[g]\n",
    "\n",
    "    # column scales: s_j = sqrt(sum W * X_tilde_j^2); guard with eps\n",
    "    for j in range(p):\n",
    "        s[j] = np.sqrt((W * X_tilde[:, j] * X_tilde[:, j]).sum()) + 1e-12\n",
    "\n",
    "    # standardized correlation at beta=0 → lambda_max\n",
    "    rho0 = np.empty(p)\n",
    "    for j in range(p):\n",
    "        xj_std = X_tilde[:, j] / s[j]\n",
    "        rho0[j] = (W * xj_std * z_tilde).sum()\n",
    "    lambda_max = np.max(np.abs(rho0))\n",
    "    lambda_calibrated = lambda_norm * lambda_max\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"lambda_max={lambda_max:.6g}  lambda_used={lambda_calibrated:.6g}\")\n",
    "\n",
    "    # ---- Outer IRLS loop ----\n",
    "    for it in range(max_outer):\n",
    "        # Profile alphas given current beta\n",
    "        alpha, eta0 = profile_alpha(beta_std)\n",
    "        eta = eta0 + alpha[g]\n",
    "        mu = np.exp(eta)\n",
    "\n",
    "        # IRLS weights/pseudo-response\n",
    "        W = mu\n",
    "        z = eta + (y - mu) / np.maximum(mu, eps)\n",
    "\n",
    "        # Weighted FE demeaning for current W, z\n",
    "        Wg = np.maximum(np.bincount(g, weights=W, minlength=m), eps)\n",
    "        Xw = X * W[:, None]\n",
    "        Xgw = np.vstack([np.bincount(g, weights=Xw[:, j], minlength=m) for j in range(p)]).T\n",
    "        zg  = np.bincount(g, weights=W * z, minlength=m)\n",
    "        X_tilde = X - Xgw[g] / Wg[g, None]\n",
    "        z_tilde = z - zg[g] / Wg[g]\n",
    "\n",
    "        # Work in standardized coordinates: X_std = X_tilde / s\n",
    "        # Keep residual r_std = z_tilde - sum_j X_std_j * beta_std_j\n",
    "        Xbeta_std = np.zeros(n)\n",
    "        for j in range(p):\n",
    "            Xbeta_std += (X_tilde[:, j] / s[j]) * beta_std[j]\n",
    "        r_std = z_tilde - Xbeta_std\n",
    "\n",
    "        # Per-feature curvature in std space: a_j = sum W * (X_tilde_j/s_j)^2 + l2 (on *unscaled* beta)\n",
    "        # Ridge on original beta translates to l2 / s_j^2 on beta_std\n",
    "        a = np.empty(p, dtype=np.float64)\n",
    "        l2_std = l2 / (s * s)\n",
    "        for j in range(p):\n",
    "            xj_std = X_tilde[:, j] / s[j]\n",
    "            a[j] = (W * xj_std * xj_std).sum() + l2_std[j]\n",
    "\n",
    "        # Coordinate descent with soft-thresholding in std space\n",
    "        for inner in range(max_inner):\n",
    "            max_delta = 0.0\n",
    "            for j in range(p):\n",
    "                xj_std = X_tilde[:, j] / s[j]\n",
    "                bj_old = beta_std[j]\n",
    "                # rho_j = sum W * xj_std * (r_std + xj_std * bj_old)\n",
    "                rho = (W * xj_std * (r_std + xj_std * bj_old)).sum()\n",
    "                # soft-threshold\n",
    "                bj_new = _soft_threshold(rho, lambda_calibrated) / a[j]\n",
    "                if bj_new != bj_old:\n",
    "                    delta = bj_new - bj_old\n",
    "                    r_std -= xj_std * delta\n",
    "                    beta_std[j] = bj_new\n",
    "                    if abs(delta) > max_delta:\n",
    "                        max_delta = abs(delta)\n",
    "            if max_delta < inner_tol * (1.0 + np.linalg.norm(beta_std, ord=np.inf)):\n",
    "                break\n",
    "\n",
    "        # Convergence test on linear predictor change\n",
    "        eta0_new = X @ (beta_std / s)\n",
    "        if np.linalg.norm(eta0_new - eta0) < tol * (1.0 + np.linalg.norm(eta0)):\n",
    "            break\n",
    "\n",
    "    # Map back to original scale\n",
    "    beta = beta_std / s\n",
    "    # Final alpha on that beta\n",
    "    eta0 = X @ beta\n",
    "    S = np.bincount(g, weights=np.exp(eta0), minlength=m)\n",
    "    alpha = np.log(np.maximum(T, eps)) - np.log(np.maximum(S, eps))\n",
    "\n",
    "    info = {\n",
    "        \"outer_iters\": it + 1,\n",
    "        \"lambda_norm\": float(lambda_norm),\n",
    "        \"lambda_max\": float(lambda_max),\n",
    "        \"lambda_used\": float(lambda_calibrated),\n",
    "        \"l2\": float(l2),\n",
    "        \"scales\": s,\n",
    "    }\n",
    "    return beta, alpha, info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta_hat_unreg, alpha_hat_unreg, _ = fit_poisson_fe_lasso(X, Y.flatten(), group_ids, 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta_hat, alpha_hat, _ = fit_poisson_fe_lasso_normalized(X, Y.flatten(), group_ids, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-0.65550382,  0.7706688 , -0.30314253,  0.35898457,  0.08699812,\n",
       "         0.06462365,  0.60495073,  0.23256946, -0.866865  ,  0.76489766]),\n",
       " array([-0.8976723 ,  1.08576294, -0.44562794,  0.51581358,  0.19999305,\n",
       "         0.16290432,  0.83137433,  0.37053112, -1.23110589,  1.06628806]))"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beta_hat, beta_hat_unreg"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
